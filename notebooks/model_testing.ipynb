{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append('../')\n",
    "from src.models.baselines import PopularRecommender, SimpleJaccard, CosineKNN\n",
    "import pandas as pd\n",
    "import random\n",
    "from src.models.models import NetStatKNN, GNNHandler\n",
    "from src.models.evaluator import Evaluator\n",
    "import src.util.tigergraph_util as tgu\n",
    "import tqdm\n",
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch_geometric.transforms as T\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the data the baselines use for training as they are non graph-based."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/out/user_subreddit.csv', header=None)\n",
    "user_subreddits = pd.read_csv('../data/out/user_subreddit.csv', header=None)\n",
    "user_subreddits.columns = ['user', 'subreddit', 'times']\n",
    "pop_subs = set(user_subreddits[['subreddit', 'times']].groupby('subreddit')['times'].count().sort_values(ascending=False).head(25).index)\n",
    "user_subreddits['subreddit times'] = list(zip(user_subreddits['times'], user_subreddits['subreddit']))\n",
    "user_subreddits = user_subreddits.groupby('user')['subreddit times'].apply(list)\n",
    "user_subreddit_map = user_subreddits.apply(lambda x: [s for c, s in sorted(x, reverse=True)]).to_dict()\n",
    "users = list(user_subreddit_map.keys())\n",
    "rand_users = random.sample(users, 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our user:\n",
    "In this notebook, we will make some subreddit recommendations for user 'lowpass'. Below are the subreddits they have interacted with sorted in order of most to least interaction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 'lowpass'\n",
    "user_subreddit_map[user]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation:\n",
    "Creating an evaluation helper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator('../data/out/test_interactions.csv', subset=500)\n",
    "at_k = [1, 3, 5, 10, 25]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline recommender models:\n",
    "The below instantiates and trains baselines, then makes a recommendation for our user:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popularity recommender:\n",
    "Recommends popular subreddits user does not belong to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_rec = PopularRecommender(df)\n",
    "pop_rec.recommend(user, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluator.precision_recall(pop_rec, at_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard similarity recommender:\n",
    "Recommends subreddits by determining what similar users (determined by Jaccard similarity) belong to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_rec = SimpleJaccard(df)\n",
    "jaccard_rec.recommend(user, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluator.precision_recall(jaccard_rec, at_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity KNN recommender:\n",
    "Recommends subreddits by determining what similar users (determined by nearest-neighbor similarity in a KNN) belong to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_rec = CosineKNN(df)\n",
    "knn_rec.recommend(user, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator.precision_recall(knn_rec, at_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interaction graph recommender models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = tgu.connection('../config/tigergraph.json')\n",
    "df = conn.getVertexDataFrame('user', select='fastrp_embedding')\n",
    "df = pd.concat([df['v_id'].to_frame(), df['fastrp_embedding'].apply(pd.Series)], axis=1)\n",
    "df.columns = ['v_id', 'pagerank', 'louvain', 'label_prop', 'degree']\n",
    "embeddings = pd.read_csv('../data/out/user.csv', header=None)\n",
    "embeddings = embeddings.rename(columns={0:'v_id'})\n",
    "embeddings['v_id'] = embeddings['v_id'].astype(str)\n",
    "df['v_id'] = df['v_id'].astype(str)\n",
    "user_data = df.merge(embeddings, on='v_id', how='inner')\n",
    "subreddit_data = pd.read_csv('../data/out/subreddit.csv', header=None)\n",
    "user_subreddit = pd.read_csv('../data/out/user_subreddit.csv', header=None)\n",
    "reddit_graph = NetStatKNN(conn)\n",
    "reddit_graph.fit(user_data, subreddit_data, user_subreddit, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centrality KNN community recommender:\n",
    "Recommends subreddits by determining what similar users (determined by nearest neighbors of the following centrality metrics: pagerank, louvain, label propagation, degree) belong to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_graph.recommend(user, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluator.precision_recall(reddit_graph, at_k, subset_size=500)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Convolution Network\n",
    "Uses a graph convolution network to predict links between users and subreddits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = pd.read_csv('../data/out/user.csv', header=None)\n",
    "subreddit = pd.read_csv('../data/out/subreddit.csv', header=None)\n",
    "user_user = pd.read_csv('../data/out/user_user.csv', header=None)\n",
    "user_user.dropna(inplace=True)\n",
    "user_subreddit = pd.read_csv('../data/out/user_subreddit.csv', header=None)\n",
    "\n",
    "unique_users = user[0].unique()\n",
    "unique_subreddits = subreddit[0].unique()\n",
    "user_map = {u:i for i, u in enumerate(unique_users)}\n",
    "rev_user_map = {i:u for u, i in user_map.items()}\n",
    "subreddit_map = {u:i for i, u in enumerate(unique_subreddits)}\n",
    "rev_subreddit_map = {i:u for u, i in subreddit_map.items()}\n",
    "\n",
    "user[0] = user[0].map(user_map)\n",
    "subreddit[0] = subreddit[0].map(subreddit_map)\n",
    "user_subreddit[0], user_subreddit[1] = user_subreddit[0].map(user_map), user_subreddit[1].map(subreddit_map)\n",
    "\n",
    "data = HeteroData()\n",
    "data['user'].node_id = torch.arange(len(unique_users))\n",
    "data['subreddit'].node_id = torch.arange(len(unique_subreddits))\n",
    "data['user'].x = torch.tensor(user.drop(columns=[0]).values)\n",
    "data['subreddit'].x = torch.tensor(subreddit.drop(columns=[0]).values)\n",
    "data['user', 'commented_in', 'subreddit'].edge_index = torch.tensor(user_subreddit.drop(columns=[2]).values).T\n",
    "data = T.ToUndirected()(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maps = (user_map, rev_user_map, subreddit_map, rev_subreddit_map)\n",
    "gnn_handler = GNNHandler(data, maps)\n",
    "gnn_handler.set_model(hidden_channels=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_handler.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.precision_recall(gnn_handler, at_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_handler.recommend('andrewsmith1986', n=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "3765b5c36215b6942432267d5a9eb97e0199a22c25b16bdeee0eec4357e615e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
